{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Finetuning Notebook\n",
    "## LLM finetuning for AES\n",
    "\n",
    "Code to finetune models served via Ollama for AES tasks\n",
    "\n",
    "### Create Slim Version of test/train set\n",
    "\n",
    "Remove metadata and just leave scores with texts"
   ],
   "id": "b0a5f455fefb740e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T15:12:55.836915Z",
     "start_time": "2025-07-10T15:12:34.398387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "name_dir = \"data\"\n",
    "file_name_train = \"persuade_corpus_2.0_train.csv\"\n",
    "file_name_test  = \"persuade_corpus_2.0_test.csv\"\n",
    "slim_columns = [\"essay_id_comp\", \"full_text\", \"holistic_essay_score\"]\n",
    "\n",
    "for split, file_name in [(\"train\", file_name_train), (\"test\", file_name_test)]:\n",
    "    fullpath = os.path.join(name_dir, file_name)\n",
    "    print(f\"loading {split} from {fullpath} …\")\n",
    "    df = pd.read_csv(fullpath)\n",
    "\n",
    "    # pick only the three\n",
    "    slim = df[slim_columns].copy()\n",
    "\n",
    "    outpath = os.path.join(name_dir, f\"slim_persuade_{split}.csv\")\n",
    "    print(f\"writing slim {split} -> {outpath}\")\n",
    "    slim.to_csv(outpath, index=False)\n"
   ],
   "id": "48e3662ec2de2a1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train from data\\persuade_corpus_2.0_train.csv …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\AppData\\Local\\Temp\\ipykernel_27472\\3584629915.py:12: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fullpath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing slim train -> data\\slim_persuade_train.csv\n",
      "loading test from data\\persuade_corpus_2.0_test.csv …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\AppData\\Local\\Temp\\ipykernel_27472\\3584629915.py:12: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fullpath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing slim test -> data\\slim_persuade_test.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load datasets",
   "id": "dfc378b95cecb6c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:53:57.439783Z",
     "start_time": "2025-07-11T12:52:50.917566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm_name  = \"mistralai/Mistral-7B-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name, use_fast=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "raw_ds = load_dataset(\"csv\", data_files={\"train\":\"data/slim_persuade_train.csv\",\n",
    "                                     \"valid\":\"data/slim_persuade_test.csv\"})\n",
    "\n",
    "# prompt for essay scoring\n",
    "def build_prompt(text:str, score:int) -> str:\n",
    "    return (\n",
    "        \"<s>[INST] You are an essay rater specializing in the evaluation of essays written by students from 6th to 12th grade. \"\n",
    "        \"Read and evaluate the essay. Assign it a score from 1 to 6, in increments of 1. \"\n",
    "        \"Your response should be only a numeric value representing the score you gave. [/INST]\\n\"\n",
    "        f\"{text.strip()}\\n\"\n",
    "        f\"</s> {score}\"\n",
    "    )\n",
    "\n",
    "# tokenize and format for pytorch\n",
    "def preprocess(ex):\n",
    "    prompt = build_prompt(ex[\"full_text\"], ex[\"holistic_essay_score\"])\n",
    "    tok    = tokenizer(prompt,\n",
    "                       truncation=True,\n",
    "                       max_length=2048,\n",
    "                       padding=\"max_length\")\n",
    "    tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
    "    return tok\n",
    "\n",
    "ds = raw_ds.map(preprocess, batched=False, remove_columns=raw_ds[\"train\"].column_names)\n",
    "ds.set_format(\"torch\")"
   ],
   "id": "adb10fa824ed7a9e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-11T12:54:17.969713Z",
     "start_time": "2025-07-11T12:53:58.566809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit            = True,\n",
    "    bnb_4bit_quant_type     = \"nf4\",\n",
    "    bnb_4bit_compute_dtype  = \"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# load model & tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True  # Gemma needs this\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# prepare PEFT (LoRA) adapter\n",
    "peft_cfg = LoraConfig(\n",
    "    r           = 8,\n",
    "    lora_alpha  = 16,\n",
    "    lora_dropout= 0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11769bdb9a6f43e28828726721f0778a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,251,431,424 || trainable%: 0.0470\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T20:32:49.770071Z",
     "start_time": "2025-07-11T12:56:20.047304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "import os, torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir            = \"checkpoints\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    eval_accumulation_steps     = 1,\n",
    "    learning_rate         = 3e-4,\n",
    "    fp16                  = True,\n",
    "    num_train_epochs      = 1,\n",
    "    max_steps             = 2000,\n",
    "    logging_steps         = 50,\n",
    "    eval_steps            = 500,\n",
    "    save_steps            = 500,\n",
    "    save_total_limit      = 2,\n",
    "    report_to             = \"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = ds[\"train\"],\n",
    "    eval_dataset  = ds[\"valid\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer     = tokenizer\n",
    ")\n",
    "\n",
    "import time\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class StepLogger(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Complete step {state.global_step}\")\n",
    "\n",
    "trainer.add_callback(StepLogger())\n",
    "\n",
    "trainer.train()"
   ],
   "id": "825dfbdb71c59186",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\AppData\\Local\\Temp\\ipykernel_1520\\1437183572.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 7:36:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.961200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.923800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.914000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.887700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.877100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.920800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.855400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.824900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.827800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.831100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.834600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.809200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:39] Complete step 100\n",
      "[14:41:08] Complete step 200\n",
      "[15:03:33] Complete step 300\n",
      "[15:26:05] Complete step 400\n",
      "[15:48:33] Complete step 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:10:59] Complete step 600\n",
      "[16:33:26] Complete step 700\n",
      "[16:55:50] Complete step 800\n",
      "[17:18:16] Complete step 900\n",
      "[17:40:40] Complete step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:02:57] Complete step 1100\n",
      "[18:25:18] Complete step 1200\n",
      "[18:47:35] Complete step 1300\n",
      "[19:09:50] Complete step 1400\n",
      "[19:32:07] Complete step 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:56:25] Complete step 1600\n",
      "[20:20:39] Complete step 1700\n",
      "[20:44:47] Complete step 1800\n",
      "[21:08:52] Complete step 1900\n",
      "[21:32:49] Complete step 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=1.8686896018981933, metrics={'train_runtime': 27389.3923, 'train_samples_per_second': 0.292, 'train_steps_per_second': 0.073, 'total_flos': 6.99650575171584e+17, 'train_loss': 1.8686896018981933, 'epoch': 0.046171782115360195})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Qwen3",
   "id": "2178f7572c1b295a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:00:40.834936Z",
     "start_time": "2025-07-17T11:53:00.877336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm_name  = \"Qwen/Qwen3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name, use_fast=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "raw_ds = load_dataset(\"csv\", data_files={\"train\":\"data/slim_persuade_train.csv\",\n",
    "                                     \"valid\":\"data/slim_persuade_test.csv\"})\n",
    "\n",
    "# prompt for essay scoring\n",
    "def build_prompt(text:str, score:int) -> str:\n",
    "    return (\n",
    "        \"<s>[INST] You are an essay rater specializing in the evaluation of essays written by students from 6th to 12th grade. \"\n",
    "        \"Read and evaluate the essay. Assign it a score from 1 to 6, in increments of 1. \"\n",
    "        \"Your response should be only a numeric value representing the score you gave. [/INST]\\n\"\n",
    "        f\"{text.strip()}\\n\"\n",
    "        f\"</s> {score}\"\n",
    "    )\n",
    "\n",
    "# tokenize and format for pytorch\n",
    "def preprocess(ex):\n",
    "    prompt = build_prompt(ex[\"full_text\"], ex[\"holistic_essay_score\"])\n",
    "    tok    = tokenizer(prompt,\n",
    "                       truncation=True,\n",
    "                       max_length=2048,\n",
    "                       padding=\"max_length\")\n",
    "    tok[\"labels\"] = tok[\"input_ids\"].copy()\n",
    "    return tok\n",
    "\n",
    "ds = raw_ds.map(preprocess, batched=False, remove_columns=raw_ds[\"train\"].column_names)\n",
    "ds.set_format(\"torch\")"
   ],
   "id": "bd1e3fba130e6707",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ed329873fe24548acd29cd3a959b2c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d8d56a8548546ff8f1c07eb31e876c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dc7f989831e4068bc0ffa51302cc318"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d259643b6bd240909df959d6f9ce5d52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/173266 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc78703d8aa042f8b785630c01abc35d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/112117 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32edd2ddfaff457099428f221f390992"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T12:27:24.181548Z",
     "start_time": "2025-07-17T12:00:56.309389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit            = True,\n",
    "    bnb_4bit_quant_type     = \"nf4\",\n",
    "    bnb_4bit_compute_dtype  = \"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# load model & tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True  # Gemma needs this\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# prepare PEFT (LoRA) adapter\n",
    "peft_cfg = LoraConfig(\n",
    "    r           = 8,\n",
    "    lora_alpha  = 16,\n",
    "    lora_dropout= 0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "adc2b800d043d9f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41c7e356a4a7494bb6bbad18550e7050"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fdb9450d182f4111a4b642e4dd59f574"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41bb5099cb8a456c811dab8896370865"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f671f724c1ce4177b1da854ab1efea57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09a1a32a04ca4a85a154a80488ed7bb4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2e9345bc52f4389bbd865a86a00346c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fe8402798b74e10bc142813f9e42044"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba678eb6948e44b88dd5f192b251c72a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb4b7cf51e33481aaf58b3c62952b4f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "faaa5e9a84a14468ab4a780a144979ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,833,856 || all params: 8,194,569,216 || trainable%: 0.0468\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T02:05:10.831476Z",
     "start_time": "2025-07-17T12:27:25.431880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "import os, torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir            = \"checkpoints/qwen3\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    eval_accumulation_steps     = 1,\n",
    "    learning_rate         = 3e-4,\n",
    "    fp16                  = True,\n",
    "    num_train_epochs      = 1,\n",
    "    max_steps             = 2000,\n",
    "    logging_steps         = 50,\n",
    "    eval_steps            = 500,\n",
    "    save_steps            = 500,\n",
    "    save_total_limit      = 3,\n",
    "    report_to             = \"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = ds[\"train\"],\n",
    "    eval_dataset  = ds[\"valid\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer     = tokenizer\n",
    ")\n",
    "\n",
    "import time\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class StepLogger(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 100 == 0:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Complete step {state.global_step}\")\n",
    "\n",
    "trainer.add_callback(StepLogger())\n",
    "\n",
    "trainer.train()"
   ],
   "id": "e05a47605e77d206",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\AppData\\Local\\Temp\\ipykernel_33708\\1993200313.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 13:37:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.084500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.033500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.018900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:08:11] Complete step 100\n",
      "[14:48:49] Complete step 200\n",
      "[15:29:26] Complete step 300\n",
      "[16:10:00] Complete step 400\n",
      "[16:50:35] Complete step 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:31:13] Complete step 600\n",
      "[18:11:52] Complete step 700\n",
      "[18:52:27] Complete step 800\n",
      "[19:33:01] Complete step 900\n",
      "[20:13:39] Complete step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:54:35] Complete step 1100\n",
      "[21:35:16] Complete step 1200\n",
      "[22:15:49] Complete step 1300\n",
      "[22:56:19] Complete step 1400\n",
      "[23:37:00] Complete step 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josea\\Documents\\Development\\StyleOverSubstance\\.venv1\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:18:33] Complete step 1600\n",
      "[01:00:08] Complete step 1700\n",
      "[01:41:48] Complete step 1800\n",
      "[02:23:28] Complete step 1900\n",
      "[03:05:10] Complete step 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=2.085228946685791, metrics={'train_runtime': 49065.0697, 'train_samples_per_second': 0.163, 'train_steps_per_second': 0.041, 'total_flos': 7.4438141804544e+17, 'train_loss': 2.085228946685791, 'epoch': 0.046171782115360195})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
